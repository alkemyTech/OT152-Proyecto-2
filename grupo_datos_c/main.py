from functools import reduce
import logging.config
import xml.etree.ElementTree as ET

from decouple import config as cfg

from modules import get_tags_and_ansid, get_tags_with_acc_answ, split_and_count_tags
from modules import count_words_in_body, bodies_with_answer_count, generate_data_dict, get_bodies_and_answer_counts

# Load logging configurations
logging.config.fileConfig(cfg('FILE_PATH'))
# Instansiate logger class
logger = logging.getLogger('Grupo_Datos_C')


def read_xml():
    """
    Loads xml file to process

            Returns: xml Element Tree
    """
    file_xml = ET.parse('posts.xml')
    return file_xml.getroot()


def chunkify(data, chunk_size=1000):
    """
    Returns a generator with the information necesary to iterate and process data

            Parameters:
                    data (xml.etree.ElementTree.Element): contains data read from xml file
                    chunk_size (int): number of data to process for each chunk
            Returns:
                    generator: contains part of that to process based on the selected chunk_size
    """
    data_lenght = len(data)

    for i in range(0, data_lenght, chunk_size):
        yield data.findall('row')[i: i + chunk_size]


def mapped_chunks_top10_tags(chunks):
    """
    Returns a map object with dictionaries containing count of tags with accepted answer for each chunk
            Parameter chunks(generator):
                    chunk of data to process
            Returns:
                    mapped (list): contains Counter dictionaries objects with tags with accepted answer for each chunk
    """
    mapped = map(get_tags_and_ansid, chunks)
    mapped = map(get_tags_with_acc_answ, mapped)
    mapped = map(split_and_count_tags, mapped)
    return mapped


def reducer_top10_tags(list_of_dict_1, list_of_dict_2):
    """
    Returns the sum of all lists of Counter dictionaries to get total of each tag with an accepted answer per chunk

            Parametes:
                    list_of_dict_1 (list): list of Counter objects generated per chunk
                    list_of_dict_2 (list): list of Counter objects generated per chunk
            Returns:
                    Counter Object with sum of tags with accepted id per chunk
    """
    return list_of_dict_1 + list_of_dict_2


def get_top_10(dictrionary):
    """
    Returns the top 10 tags with accepted answers as a list of tuples
        Parameter:
                dictrionary (Counter object): contains final count of all tags with accepted answer id
        Returns:
                sorted list with top 10 tags with accepted answer id in descending order
    """
    return sorted(dictrionary.items(), key=lambda item: item[1], reverse=True)[:10]


def top_10_tags():
    # Read xml file and load data to root

    root = read_xml()

    # Load data by chunks of 1000 data each
    data_chunks = chunkify(root, 1000)
    # Perform first step of map reduce
    mapped_chunks = mapped_chunks_top10_tags(data_chunks)
    # Perform second and las step, sum of all Counter objects generated by mapped_chunks function
    reduced = reduce(reducer_top10_tags, mapped_chunks)
    # Get top 10 tags with accepted answer id and display
    top_10 = get_top_10(reduced)
    logger.info("\t Top 10 tags with accepted answer id: ")
    for tag, count in top_10:
        logger.info(f'\t Tag: {tag}, Count: {count}')


def mapped_chunks_words_per_answercount(data_chunks):
    """
    Returns a map object with the dictionaries containg id, word count, answer count and word/answer count ratio
            Parameter chunks(generator):
                    chunk of data to process
            Returns:
                    mapped (map object): contains dictionaries objects with tags with accepted answer for each chunk
    """
    mapped = map(get_bodies_and_answer_counts, data_chunks)
    mapped = map(bodies_with_answer_count, mapped)
    mapped = map(count_words_in_body, mapped)
    mapped = map(generate_data_dict, mapped)
    return mapped


def reducer_word_per_answer_ratio(data_dict_1, data_dict_2):
    """
    Returns the sum of all lists of Counter dictionaries to get total of each tag with an accepted answer per chunk

            Parametes:
                    data_dict_1 (dict): dictionary with data processed
                    data_dict_2 (dict): dictionary with data processed
            Returns:
                    data_dict_1 (dict) dictionary updated with data_dict_2
    """
    data_dict_1.update(data_dict_2)
    return data_dict_1


def display_post_wordanswer_ratio(data, post_id=1):
    """
    Display post id, word count, answer count and words/answers ratio

            Parametes:
                    data (dict): dictionary with data processed
    """
    try:
        tabs = '\t' * 8
        logger.info(f'\t Post N{post_id} has: \n'
                    f'{tabs} Word count: {data[post_id]["Word count"]}\n'
                    f'{tabs} Answer count: {data[post_id]["Answer count"]}\n'
                    f'{tabs} Word/Answers ratio: {data[post_id]["Words per answers ratio"]}\n')

    except KeyError:
        logger.warning(' Post id is not in dictionary, post probably has not Answer count')


def word_per_answer_ratio():
    root = read_xml()
    # Load data by chunks of 1000 data each
    data_chunks = chunkify(root, 4000)
    # Perform first step of map reduce
    mapped_chunks = mapped_chunks_words_per_answercount(data_chunks)
    reduced = reduce(reducer_word_per_answer_ratio, mapped_chunks)
    display_post_wordanswer_ratio(reduced, post_id=6)


# Run main script
if __name__ == '__main__':
    # Get top 10 tags with accepted answer id
    top_10_tags()
    word_per_answer_ratio()
